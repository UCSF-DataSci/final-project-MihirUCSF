{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d62b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check readable data\n",
    "# set the dataset path \n",
    "dataset_path = r\"C:/Users/mihir/OneDrive/Desktop/EPI233/multilevel-monitoring-of-activity-and-sleep-in-healthy-people-1.0.0/data\"\n",
    "\n",
    "print(f\"checking dataset at path: {dataset_path}\")\n",
    "\n",
    "# check if main directory exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"error: dataset path '{dataset_path}' does not exist.\")\n",
    "else:\n",
    "    print(f\"success: dataset base directory found.\")\n",
    "    \n",
    "    # list all items in the directory to understand structure\n",
    "    all_items = os.listdir(dataset_path)\n",
    "    print(f\"\\nfound {len(all_items)} items in the base directory:\")\n",
    "    \n",
    "    # count directories and files\n",
    "    dirs = [item for item in all_items if os.path.isdir(os.path.join(dataset_path, item))]\n",
    "    files = [item for item in all_items if os.path.isfile(os.path.join(dataset_path, item))]\n",
    "    \n",
    "    print(f\"- {len(dirs)} directories: {', '.join(dirs[:5])}\" + (\"...\" if len(dirs) > 5 else \"\"))\n",
    "    print(f\"- {len(files)} files: {', '.join(files[:5])}\" + (\"...\" if len(files) > 5 else \"\"))\n",
    "    \n",
    "    # find user directories (using lowercase \"user_\" pattern as observed in the error message)\n",
    "    user_dirs = glob(os.path.join(dataset_path, \"user_*\"))\n",
    "    \n",
    "    if not user_dirs:\n",
    "        # if no \"user_XX\" directories found, check if we're one level too high/low\n",
    "        potential_data_dirs = [d for d in dirs if \"data\" in d.lower()]\n",
    "        if potential_data_dirs:\n",
    "            print(f\"\\nno user directories found, but found potential data directories: {potential_data_dirs}\")\n",
    "            for potential_dir in potential_data_dirs:\n",
    "                full_path = os.path.join(dataset_path, potential_dir)\n",
    "                sub_user_dirs = glob(os.path.join(full_path, \"user_*\"))\n",
    "                if sub_user_dirs:\n",
    "                    print(f\"found {len(sub_user_dirs)} user directories in '{potential_dir}'\")\n",
    "                    user_dirs = sub_user_dirs\n",
    "                    dataset_path = full_path\n",
    "                    break\n",
    "    \n",
    "    if not user_dirs:\n",
    "        print(\"\\nwarning: no user directories found with pattern 'user_*'\")\n",
    "        print(\"searching for any directories that might contain the data...\")\n",
    "        \n",
    "        # try to find any directory that might contain our expected files\n",
    "        expected_files = [\"sleep.csv\", \"rr.csv\", \"user_info.csv\"]\n",
    "        for root, dirs, files in os.walk(dataset_path):\n",
    "            for filename in files:\n",
    "                if filename.lower() in [f.lower() for f in expected_files]:\n",
    "                    print(f\"found expected file {filename} in {root}\")\n",
    "    else:\n",
    "        print(f\"\\nfound {len(user_dirs)} user directories.\")\n",
    "        \n",
    "        # define expected files (using lowercase as files might be lowercase)\n",
    "        expected_files = [\n",
    "            \"user_info.csv\",\n",
    "            \"sleep.csv\", \n",
    "            \"RR.csv\",\n",
    "            \"questionnaire.csv\",\n",
    "            \"Activity.csv\", \n",
    "            \"Actigraph.csv\",\n",
    "            \"saliva.csv\"\n",
    "        ]\n",
    "        \n",
    "        # create a report dictionary\n",
    "        file_report = {file: {\"present\": 0, \"readable\": 0} for file in expected_files}\n",
    "        user_report = {}\n",
    "        \n",
    "        print(\"\\nvalidating individual user directories:\")\n",
    "        \n",
    "        # check a subset of users for detailed inspection (to avoid long output)\n",
    "        sample_users = user_dirs[:3] if len(user_dirs) > 3 else user_dirs\n",
    "        \n",
    "        for user_dir in sample_users:\n",
    "            user_id = os.path.basename(user_dir)\n",
    "            print(f\"\\n  checking {user_id}...\")\n",
    "            user_report[user_id] = {\"files_present\": 0, \"files_readable\": 0}\n",
    "            \n",
    "            for expected_file in expected_files:\n",
    "                file_path = os.path.join(user_dir, expected_file)\n",
    "                \n",
    "                # check if file exists\n",
    "                if os.path.exists(file_path):\n",
    "                    file_report[expected_file][\"present\"] += 1\n",
    "                    user_report[user_id][\"files_present\"] += 1\n",
    "                    print(f\"    ✓ found {expected_file}\")\n",
    "                    \n",
    "                    # try reading the file\n",
    "                    try:\n",
    "                        if expected_file.endswith('.csv'):\n",
    "                            # read just a few rows to validate\n",
    "                            df = pd.read_csv(file_path, nrows=5)\n",
    "                            row_count = len(df)\n",
    "                            col_count = len(df.columns)\n",
    "                            file_report[expected_file][\"readable\"] += 1\n",
    "                            user_report[user_id][\"files_readable\"] += 1\n",
    "                            print(f\"      ✓ successfully read {expected_file}: {row_count} rows × {col_count} columns\")\n",
    "                            print(f\"      ✓ columns: {', '.join(df.columns[:3])}...\" if col_count > 3 else f\"      ✓ columns: {', '.join(df.columns)}\")\n",
    "                        else:\n",
    "                            # for non-CSV files\n",
    "                            with open(file_path, 'r') as f:\n",
    "                                lines = f.readlines(100)  # read just a few lines\n",
    "                            file_report[expected_file][\"readable\"] += 1\n",
    "                            user_report[user_id][\"files_readable\"] += 1\n",
    "                            print(f\"      ✓ successfully read {expected_file}: {len(lines)} lines\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"      ✗ error reading {expected_file}: {str(e)}\")\n",
    "                else:\n",
    "                    # try with capitalized filename as well\n",
    "                    cap_file_path = os.path.join(user_dir, expected_file.capitalize())\n",
    "                    if os.path.exists(cap_file_path):\n",
    "                        file_report[expected_file][\"present\"] += 1\n",
    "                        user_report[user_id][\"files_present\"] += 1\n",
    "                        print(f\"    ✓ found {expected_file} (capitalized)\")\n",
    "                        \n",
    "                        try:\n",
    "                            if cap_file_path.endswith('.csv'):\n",
    "                                df = pd.read_csv(cap_file_path, nrows=5)\n",
    "                                row_count = len(df)\n",
    "                                col_count = len(df.columns)\n",
    "                                file_report[expected_file][\"readable\"] += 1\n",
    "                                user_report[user_id][\"files_readable\"] += 1\n",
    "                                print(f\"      ✓ successfully read {expected_file}: {row_count} rows × {col_count} columns\")\n",
    "                                print(f\"      ✓ columns: {', '.join(df.columns[:3])}...\" if col_count > 3 else f\"      ✓ columns: {', '.join(df.columns)}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"      ✗ error reading {expected_file}: {str(e)}\")\n",
    "                    else:\n",
    "                        # check for file with .CSV extension instead of .csv\n",
    "                        upper_ext_file_path = os.path.join(user_dir, expected_file[:-4] + \".CSV\")\n",
    "                        if os.path.exists(upper_ext_file_path):\n",
    "                            file_report[expected_file][\"present\"] += 1\n",
    "                            user_report[user_id][\"files_present\"] += 1\n",
    "                            print(f\"    ✓ found {expected_file} (with .CSV extension)\")\n",
    "                            \n",
    "                            try:\n",
    "                                df = pd.read_csv(upper_ext_file_path, nrows=5)\n",
    "                                row_count = len(df)\n",
    "                                col_count = len(df.columns)\n",
    "                                file_report[expected_file][\"readable\"] += 1\n",
    "                                user_report[user_id][\"files_readable\"] += 1\n",
    "                                print(f\"      ✓ successfully read {expected_file}: {row_count} rows × {col_count} columns\")\n",
    "                                print(f\"      ✓ columns: {', '.join(df.columns[:3])}...\" if col_count > 3 else f\"      ✓ columns: {', '.join(df.columns)}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"      ✗ error reading {expected_file}: {str(e)}\")\n",
    "                        else:\n",
    "                            print(f\"    ✗ missing {expected_file}\")\n",
    "        \n",
    "        # now check all remaining users without detailed output\n",
    "        if len(user_dirs) > 3:\n",
    "            print(f\"\\nchecking remaining {len(user_dirs) - len(sample_users)} users...\")\n",
    "            \n",
    "            for user_dir in user_dirs[3:]:\n",
    "                user_id = os.path.basename(user_dir)\n",
    "                user_report[user_id] = {\"files_present\": 0, \"files_readable\": 0}\n",
    "                \n",
    "                for expected_file in expected_files:\n",
    "                    file_path = os.path.join(user_dir, expected_file)\n",
    "                    cap_file_path = os.path.join(user_dir, expected_file.capitalize())\n",
    "                    upper_ext_file_path = os.path.join(user_dir, expected_file[:-4] + \".CSV\")\n",
    "                    \n",
    "                    # check all possible file path variants\n",
    "                    for path in [file_path, cap_file_path, upper_ext_file_path]:\n",
    "                        if os.path.exists(path):\n",
    "                            file_report[expected_file][\"present\"] += 1\n",
    "                            user_report[user_id][\"files_present\"] += 1\n",
    "                            \n",
    "                            # try reading the file\n",
    "                            try:\n",
    "                                if path.lower().endswith('.csv'):\n",
    "                                    df = pd.read_csv(path, nrows=5)\n",
    "                                    file_report[expected_file][\"readable\"] += 1\n",
    "                                    user_report[user_id][\"files_readable\"] += 1\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                            \n",
    "                            break  # file found, move to next expected file\n",
    "        \n",
    "        # generate summary report\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"dataset validation summary\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nfile availability (across {len(user_dirs)} users):\")\n",
    "        for file, stats in file_report.items():\n",
    "            present_pct = (stats[\"present\"] / len(user_dirs)) * 100\n",
    "            readable_pct = (stats[\"readable\"] / len(user_dirs)) * 100\n",
    "            print(f\"- {file:<15}: present: {stats['present']}/{len(user_dirs)} ({present_pct:.1f}%), readable: {stats['readable']}/{len(user_dirs)} ({readable_pct:.1f}%)\")\n",
    "        \n",
    "        # check which users have all files\n",
    "        complete_users = [user for user, stats in user_report.items() if stats[\"files_present\"] == len(expected_files)]\n",
    "        readable_users = [user for user, stats in user_report.items() if stats[\"files_readable\"] == len(expected_files)]\n",
    "        \n",
    "        print(f\"\\nuser completeness:\")\n",
    "        print(f\"- users with all files present: {len(complete_users)}/{len(user_dirs)} ({(len(complete_users)/len(user_dirs))*100:.1f}%)\")\n",
    "        print(f\"- users with all files readable: {len(readable_users)}/{len(user_dirs)} ({(len(readable_users)/len(user_dirs))*100:.1f}%)\")\n",
    "        \n",
    "        if len(readable_users) < len(user_dirs):\n",
    "            print(\"\\nusers with incomplete/unreadable data:\")\n",
    "            incomplete_users = [user for user, stats in user_report.items() if stats[\"files_readable\"] < len(expected_files)]\n",
    "            for user in incomplete_users[:5]:  # show first 5 incomplete users\n",
    "                print(f\"- {user}: {user_report[user]['files_readable']}/{len(expected_files)} readable files\")\n",
    "            if len(incomplete_users) > 5:\n",
    "                print(f\"  ... and {len(incomplete_users) - 5} more\")\n",
    "                \n",
    "        # overall validation result\n",
    "        if len(readable_users) > 0:\n",
    "            print(\"\\nvalidation result: passed ✓\")\n",
    "            print(f\"dataset contains {len(readable_users)} fully usable user directories.\")\n",
    "        else:\n",
    "            print(\"\\nvalidation result: failed ✗\")\n",
    "            print(\"no user directories with complete readable data found.\")\n",
    "\n",
    "        # try reading specific files to ensure data format is as expected\n",
    "        print(\"\\nperforming targeted file content validation...\")\n",
    "        \n",
    "        if readable_users:\n",
    "            # use the first complete user for validation\n",
    "            sample_user_dir = os.path.join(dataset_path, readable_users[0])\n",
    "            \n",
    "            # function to find file with case-insensitive matching\n",
    "            def find_file(directory, filename):\n",
    "                for f in os.listdir(directory):\n",
    "                    if f.lower() == filename.lower():\n",
    "                        return os.path.join(directory, f)\n",
    "                return None\n",
    "            \n",
    "            # 1. validate sleep.csv structure\n",
    "            sleep_path = find_file(sample_user_dir, \"sleep.csv\")\n",
    "            if sleep_path:\n",
    "                try:\n",
    "                    sleep_df = pd.read_csv(sleep_path)\n",
    "                    # convert column names to lowercase for comparison\n",
    "                    cols_lower = [col.lower() for col in sleep_df.columns]\n",
    "                    expected_sleep_cols = ['in bed date', 'in bed time', 'out bed date', 'out bed time']\n",
    "                    missing_cols = [col for col in expected_sleep_cols if col not in cols_lower]\n",
    "                    \n",
    "                    if not missing_cols:\n",
    "                        print(\"✓ sleep.csv has the expected column structure\")\n",
    "                    else:\n",
    "                        print(f\"✗ sleep.csv is missing expected columns: {missing_cols}\")\n",
    "                        print(f\"  available columns: {sleep_df.columns.tolist()}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ error validating sleep.csv: {str(e)}\")\n",
    "            else:\n",
    "                print(\"✗ could not find sleep.csv for validation\")\n",
    "            \n",
    "            # 2. validate rr.csv structure\n",
    "            rr_path = find_file(sample_user_dir, \"rr.csv\")\n",
    "            if rr_path:\n",
    "                try:\n",
    "                    rr_df = pd.read_csv(rr_path)\n",
    "                    cols_lower = [col.lower() for col in rr_df.columns]\n",
    "                    expected_rr_cols = ['ibi_s', 'day', 'time']\n",
    "                    missing_cols = [col for col in expected_rr_cols if col not in cols_lower]\n",
    "                    \n",
    "                    if not missing_cols:\n",
    "                        print(\"✓ rr.csv has the expected column structure\")\n",
    "                    else:\n",
    "                        print(f\"✗ rr.csv is missing expected columns: {missing_cols}\")\n",
    "                        print(f\"  available columns: {rr_df.columns.tolist()}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ error validating rr.csv: {str(e)}\")\n",
    "            else:\n",
    "                print(\"✗ could not find rr.csv for validation\")\n",
    "                \n",
    "            # 3. validate actigraph.csv (sample only due to potential size)\n",
    "            actigraph_path = find_file(sample_user_dir, \"actigraph.csv\")\n",
    "            if actigraph_path:\n",
    "                try:\n",
    "                    actigraph_df = pd.read_csv(actigraph_path, nrows=10)\n",
    "                    cols_lower = [col.lower() for col in actigraph_df.columns]\n",
    "                    expected_actigraph_cols = ['axis1', 'axis2', 'axis3', 'steps']\n",
    "                    missing_cols = [col for col in expected_actigraph_cols if col not in cols_lower]\n",
    "                    \n",
    "                    if not missing_cols:\n",
    "                        print(\"✓ actigraph.csv has the expected column structure\")\n",
    "                    else:\n",
    "                        print(f\"✗ actigraph.csv is missing expected columns: {missing_cols}\")\n",
    "                        print(f\"  available columns: {actigraph_df.columns.tolist()}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ error validating actigraph.csv: {str(e)}\")\n",
    "            else:\n",
    "                print(\"✗ could not find actigraph.csv for validation\")\n",
    "\n",
    "print(\"\\nvalidation process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc40f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'petroff10', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n"
     ]
    }
   ],
   "source": [
    "# matplotlib\n",
    "print(plt.style.available)\n",
    "plt.style.use('seaborn-v0_8-pastel')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_context(\"notebook\", font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "base_path = \"C:/Users/mihir/OneDrive/Desktop/EPI233/multilevel-monitoring-of-activity-and-sleep-in-healthy-people-1.0.0/data\"\n",
    "\n",
    "# function to find file with case-insensitive matching\n",
    "def find_file(directory, filename):\n",
    "    \"\"\"finds a file in a directory regardless of case\"\"\"\n",
    "    for f in os.listdir(directory):\n",
    "        if f.lower() == filename.lower():\n",
    "            return os.path.join(directory, f)\n",
    "    return None\n",
    "\n",
    "# function to flexibly parse time strings in various formats\n",
    "def parse_time_flexibly(day, time_str):\n",
    "    if pd.isna(day) or pd.isna(time_str):\n",
    "        return None\n",
    "        \n",
    "    # convert day to string if it's not already\n",
    "    day_str = str(day)\n",
    "    \n",
    "    # try different time parsing approaches\n",
    "    try:\n",
    "        # first try direct parsing with day + time\n",
    "        return pd.to_datetime(f\"{day_str} {time_str}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # try standardizing the time format first\n",
    "    try:\n",
    "        # handle cases with just hours and minutes (0:00 or 00:00)\n",
    "        if len(time_str.split(':')) == 2:\n",
    "            # ensure hours are zero-padded\n",
    "            hours, minutes = time_str.split(':')\n",
    "            time_str = f\"{int(hours):02d}:{minutes}\"\n",
    "            return pd.to_datetime(f\"{day_str} {time_str}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # try another approach - extract components manually\n",
    "    try:\n",
    "        time_parts = time_str.split(':')\n",
    "        if len(time_parts) >= 2:\n",
    "            # get hours and minutes\n",
    "            hour = int(time_parts[0])\n",
    "            minute = int(time_parts[1])\n",
    "            \n",
    "            # handle seconds if present\n",
    "            second = 0\n",
    "            if len(time_parts) >= 3:\n",
    "                second = int(time_parts[2])\n",
    "            \n",
    "            # create datetime\n",
    "            from datetime import datetime\n",
    "            return pd.to_datetime(datetime(2000, 1, int(day), hour, minute, second))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# function to load data for a single user\n",
    "def load_user_data(user_id):\n",
    "    # format user_id according to directory naming in dataset\n",
    "    if isinstance(user_id, int):\n",
    "        user_id = f\"user_{user_id}\"\n",
    "    elif not user_id.startswith(\"user_\"):\n",
    "        user_id = f\"user_{user_id}\"\n",
    "        \n",
    "    user_path = os.path.join(base_path, user_id)\n",
    "    \n",
    "    if not os.path.exists(user_path):\n",
    "        print(f\"warning: user directory '{user_path}' not found\")\n",
    "        return {}\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # files to load with potential case variations\n",
    "    files_to_load = {\n",
    "        'user_info': 'user_info.csv',\n",
    "        'sleep': 'sleep.csv',\n",
    "        'rr': 'RR.csv',\n",
    "        'questionnaire': 'questionnaire.csv',\n",
    "        'activity': 'Activity.csv',\n",
    "        'actigraph': 'Actigraph.csv',\n",
    "        'saliva': 'saliva.csv'\n",
    "    }\n",
    "    \n",
    "    # load each file\n",
    "    for key, filename in files_to_load.items():\n",
    "        # find the file path regardless of case\n",
    "        file_path = find_file(user_path, filename)\n",
    "        \n",
    "        if file_path:\n",
    "            try:\n",
    "                # for actigraph, sample rows due to potential size\n",
    "                if key == 'actigraph':\n",
    "                    data[key] = pd.read_csv(file_path, nrows=10000)\n",
    "                    data['actigraph_full_path'] = file_path\n",
    "                else:\n",
    "                    data[key] = pd.read_csv(file_path)\n",
    "                print(f\"  loaded {os.path.basename(file_path)} for {user_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  error loading {filename} for {user_id}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"  warning: {filename} not found for {user_id}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# function to discover all available users in the dataset\n",
    "def get_all_user_ids():\n",
    "    user_dirs = sorted(glob(os.path.join(base_path, \"user_*\")))\n",
    "    user_ids = [os.path.basename(d) for d in user_dirs]\n",
    "    return user_ids\n",
    "\n",
    "# load data for all users\n",
    "def load_all_users():\n",
    "    user_ids = get_all_user_ids()\n",
    "    print(f\"found {len(user_ids)} users in the dataset\")\n",
    "    \n",
    "    all_users_data = []\n",
    "    \n",
    "    # aggregate dataframes\n",
    "    combined_data = {\n",
    "        'user_info': [],\n",
    "        'sleep': [],\n",
    "        'questionnaire': [],\n",
    "        'saliva': [],\n",
    "        'rr': [],\n",
    "        'activity': [],\n",
    "        'actigraph': []\n",
    "    }\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        print(f\"loading data for {user_id}...\")\n",
    "        user_data = load_user_data(user_id)\n",
    "        \n",
    "        # add user id to each dataframe for tracking\n",
    "        for key in user_data:\n",
    "            if isinstance(user_data[key], pd.DataFrame):\n",
    "                if 'user_id' not in user_data[key].columns:\n",
    "                    user_data[key]['user_id'] = user_id\n",
    "        \n",
    "        # append to combined dataframes for specific files\n",
    "        for key in combined_data:\n",
    "            if key in user_data and isinstance(user_data[key], pd.DataFrame) and not user_data[key].empty:\n",
    "                combined_data[key].append(user_data[key])\n",
    "        \n",
    "        all_users_data.append(user_data)\n",
    "    \n",
    "    # concatenate combined dataframes\n",
    "    for key in combined_data:\n",
    "        if combined_data[key]:\n",
    "            combined_data[key] = pd.concat(combined_data[key], ignore_index=True)\n",
    "        else:\n",
    "            combined_data[key] = pd.DataFrame()\n",
    "    \n",
    "    return {'combined': combined_data, 'individual': all_users_data}\n",
    "\n",
    "# function to check for missing values\n",
    "def check_missing_values(data):\n",
    "    missing_info = {}\n",
    "    \n",
    "    for key, df in data.items():\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            missing = df.isnull().sum()\n",
    "            missing_percent = (missing / len(df)) * 100\n",
    "            missing_info[key] = pd.DataFrame({\n",
    "                'missing values': missing,\n",
    "                'percent missing': missing_percent\n",
    "            }).reset_index().rename(columns={'index': 'column'})\n",
    "    \n",
    "    return missing_info\n",
    "\n",
    "# function to extract hrv features from rr intervals\n",
    "def extract_hrv_features(rr_data, window_size=300):\n",
    "    \"\"\"\n",
    "    extracts heart rate variability features from rr interval data.\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    rr_data : pandas.dataframe\n",
    "        dataframe containing rr interval data\n",
    "    window_size : int, optional\n",
    "        window size in seconds for feature extraction (default: 300 seconds = 5 minutes)\n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    pandas.dataframe\n",
    "        dataframe with hrv features\n",
    "    \"\"\"\n",
    "    if rr_data.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # convert time to datetime if it's not already\n",
    "    if 'time' in rr_data.columns and not pd.api.types.is_datetime64_any_dtype(rr_data['time']):\n",
    "        # Create a new datetime column using our flexible parser\n",
    "        datetime_values = []\n",
    "        for i, row in rr_data.iterrows():\n",
    "            dt = parse_time_flexibly(row['day'], row['time'])\n",
    "            datetime_values.append(dt)\n",
    "        \n",
    "        rr_data['datetime'] = datetime_values\n",
    "        \n",
    "        # Remove rows where datetime conversion failed\n",
    "        if rr_data['datetime'].isna().any():\n",
    "            print(f\"warning: {rr_data['datetime'].isna().sum()} rows with invalid datetime were removed\")\n",
    "            rr_data = rr_data.dropna(subset=['datetime'])\n",
    "            \n",
    "        if rr_data.empty:\n",
    "            print(\"error: all datetime conversions failed\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # sort by datetime\n",
    "    try:\n",
    "        rr_data = rr_data.sort_values('datetime')\n",
    "    except Exception as e:\n",
    "        print(f\"error sorting by datetime: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # initialize lists to store features\n",
    "    windows = []\n",
    "    mean_rr = []\n",
    "    sdnn = []\n",
    "    rmssd = []\n",
    "    pnn50 = []\n",
    "    \n",
    "    # set the start time\n",
    "    start_time = rr_data['datetime'].min()\n",
    "    end_time = start_time + pd.Timedelta(seconds=window_size)\n",
    "    \n",
    "    # loop through windows\n",
    "    while end_time <= rr_data['datetime'].max():\n",
    "        # get rr intervals in the current window\n",
    "        window_data = rr_data[(rr_data['datetime'] >= start_time) & (rr_data['datetime'] < end_time)]\n",
    "        \n",
    "        if len(window_data) > 1:  # need at least 2 intervals for calculations\n",
    "            rr_intervals = window_data['ibi_s'].values * 1000  # convert to milliseconds\n",
    "            \n",
    "            # calculate features\n",
    "            windows.append(start_time)\n",
    "            mean_rr.append(np.mean(rr_intervals))\n",
    "            sdnn.append(np.std(rr_intervals))\n",
    "            \n",
    "            # calculate rmssd\n",
    "            diffs = np.diff(rr_intervals)\n",
    "            rmssd.append(np.sqrt(np.mean(diffs**2)))\n",
    "            \n",
    "            # calculate pnn50\n",
    "            pnn50.append(np.sum(np.abs(diffs) > 50) / len(diffs) * 100)\n",
    "        \n",
    "        # move to the next window\n",
    "        start_time = end_time\n",
    "        end_time = start_time + pd.Timedelta(seconds=window_size)\n",
    "    \n",
    "    # create dataframe with features\n",
    "    hrv_features = pd.DataFrame({\n",
    "        'timestamp': windows,\n",
    "        'mean_rr': mean_rr,\n",
    "        'sdnn': sdnn,\n",
    "        'rmssd': rmssd,\n",
    "        'pnn50': pnn50\n",
    "    })\n",
    "    \n",
    "    return hrv_features\n",
    "\n",
    "# function to preprocess sleep data\n",
    "def preprocess_sleep_data(sleep_df):\n",
    "    \"\"\"\n",
    "    preprocesses sleep data by converting time columns to datetime and calculating additional metrics.\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    sleep_df : pandas.dataframe\n",
    "        dataframe containing sleep data\n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    pandas.dataframe\n",
    "        processed sleep data\n",
    "    \"\"\"\n",
    "    if sleep_df.empty:\n",
    "        return sleep_df\n",
    "    \n",
    "    # make a copy to avoid modifying the original\n",
    "    df = sleep_df.copy()\n",
    "    \n",
    "    # check for expected column patterns (case insensitive)\n",
    "    col_map = {}\n",
    "    time_cols = ['in bed time', 'out bed time', 'onset time']\n",
    "    date_cols = ['in bed date', 'out bed date', 'onset date']\n",
    "    \n",
    "    # map actual column names to expected names (case-insensitive)\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        for expected in time_cols + date_cols:\n",
    "            if expected in col_lower:\n",
    "                col_map[expected] = col\n",
    "    \n",
    "    # convert time columns to datetime\n",
    "    for time_col, date_col in zip(time_cols, date_cols):\n",
    "        if time_col in col_map and date_col in col_map:\n",
    "            # create datetime column using flexible parser\n",
    "            datetime_col_name = f'{time_col}_datetime'\n",
    "            datetime_values = []\n",
    "            \n",
    "            for i, row in df.iterrows():\n",
    "                date_val = row[col_map[date_col]]\n",
    "                time_val = row[col_map[time_col]]\n",
    "                dt = parse_time_flexibly(date_val, time_val)\n",
    "                datetime_values.append(dt)\n",
    "            \n",
    "            df[datetime_col_name] = datetime_values\n",
    "    \n",
    "    # calculate sleep duration in hours\n",
    "    if 'onset time_datetime' in df.columns and 'out bed time_datetime' in df.columns:\n",
    "        valid_mask = ~(df['onset time_datetime'].isna() | df['out bed time_datetime'].isna())\n",
    "        if valid_mask.any():\n",
    "            df.loc[valid_mask, 'sleep_duration_hours'] = (\n",
    "                df.loc[valid_mask, 'out bed time_datetime'] - \n",
    "                df.loc[valid_mask, 'onset time_datetime']\n",
    "            ).dt.total_seconds() / 3600\n",
    "    \n",
    "    # calculate sleep efficiency as percentage - need to find TST and minutes in bed columns\n",
    "    tst_col = None\n",
    "    minutes_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'sleep time' in col_lower or 'tst' in col_lower:\n",
    "            tst_col = col\n",
    "        if 'minutes in bed' in col_lower:\n",
    "            minutes_col = col\n",
    "    \n",
    "    if tst_col and minutes_col:\n",
    "        valid_mask = ~(df[tst_col].isna() | df[minutes_col].isna()) & (df[minutes_col] > 0)\n",
    "        if valid_mask.any():\n",
    "            df.loc[valid_mask, 'sleep_efficiency'] = (\n",
    "                df.loc[valid_mask, tst_col] / df.loc[valid_mask, minutes_col]\n",
    "            ) * 100\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function to preprocess activity data\n",
    "def preprocess_activity_data(activity_df):\n",
    "    \"\"\"\n",
    "    preprocesses activity data by creating datetime columns and calculating durations.\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    activity_df : pandas.dataframe\n",
    "        dataframe containing activity data\n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    pandas.dataframe\n",
    "        processed activity data\n",
    "    \"\"\"\n",
    "    if activity_df.empty:\n",
    "        return activity_df\n",
    "    \n",
    "    # make a copy to avoid modifying the original\n",
    "    df = activity_df.copy()\n",
    "    \n",
    "    # identify column names (case insensitive)\n",
    "    start_col = None\n",
    "    end_col = None\n",
    "    day_col = None\n",
    "    activity_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'start' in col_lower:\n",
    "            start_col = col\n",
    "        elif 'end' in col_lower:\n",
    "            end_col = col\n",
    "        elif 'day' in col_lower:\n",
    "            day_col = col\n",
    "        elif 'activity' in col_lower:\n",
    "            activity_col = col\n",
    "    \n",
    "    # convert start and end times to datetime using flexible parser\n",
    "    if start_col and day_col:\n",
    "        start_datetime_values = []\n",
    "        for i, row in df.iterrows():\n",
    "            dt = parse_time_flexibly(row[day_col], row[start_col])\n",
    "            start_datetime_values.append(dt)\n",
    "        df['start_datetime'] = start_datetime_values\n",
    "    \n",
    "    if end_col and day_col:\n",
    "        end_datetime_values = []\n",
    "        for i, row in df.iterrows():\n",
    "            dt = parse_time_flexibly(row[day_col], row[end_col])\n",
    "            end_datetime_values.append(dt)\n",
    "        df['end_datetime'] = end_datetime_values\n",
    "    \n",
    "    # calculate activity duration in minutes\n",
    "    if 'start_datetime' in df.columns and 'end_datetime' in df.columns:\n",
    "        valid_mask = ~(df['start_datetime'].isna() | df['end_datetime'].isna())\n",
    "        if valid_mask.any():\n",
    "            df.loc[valid_mask, 'duration_minutes'] = (\n",
    "                df.loc[valid_mask, 'end_datetime'] - \n",
    "                df.loc[valid_mask, 'start_datetime']\n",
    "            ).dt.total_seconds() / 60\n",
    "    \n",
    "    # map activity codes to descriptive names if activity column exists\n",
    "    activity_mapping = {\n",
    "        1: 'sleeping',\n",
    "        2: 'laying_down',\n",
    "        3: 'sitting',\n",
    "        4: 'light_movement',\n",
    "        5: 'medium_movement',\n",
    "        6: 'heavy_movement',\n",
    "        7: 'eating',\n",
    "        8: 'small_screen_usage',\n",
    "        9: 'large_screen_usage',\n",
    "        10: 'caffeinated_drink',\n",
    "        11: 'smoking',\n",
    "        12: 'alcohol'\n",
    "    }\n",
    "    \n",
    "    if activity_col:\n",
    "        df['activity_name'] = df[activity_col].map(activity_mapping)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function to preprocess actigraph data\n",
    "def preprocess_actigraph_data(actigraph_df):\n",
    "    \"\"\"\n",
    "    preprocesses actigraph data by creating datetime columns and calculating additional metrics.\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    actigraph_df : pandas.dataframe\n",
    "        dataframe containing actigraph data\n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    pandas.dataframe\n",
    "        processed actigraph data\n",
    "    \"\"\"\n",
    "    if actigraph_df.empty:\n",
    "        return actigraph_df\n",
    "    \n",
    "    # make a copy to avoid modifying the original\n",
    "    df = actigraph_df.copy()\n",
    "    \n",
    "    # identify column names (case insensitive)\n",
    "    time_col = None\n",
    "    day_col = None\n",
    "    axis_cols = []\n",
    "    position_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'time' in col_lower and not 'timestamp' in col_lower:\n",
    "            time_col = col\n",
    "        elif 'day' in col_lower:\n",
    "            day_col = col\n",
    "        elif 'axis' in col_lower:\n",
    "            axis_cols.append(col)\n",
    "        elif 'inclinometer' in col_lower:\n",
    "            position_cols.append(col)\n",
    "    \n",
    "    # convert time to datetime using flexible parser\n",
    "    if time_col and day_col:\n",
    "        datetime_values = []\n",
    "        # For large dataframes, process in chunks to avoid memory issues\n",
    "        chunk_size = 10000\n",
    "        total_rows = len(df)\n",
    "        \n",
    "        print(f\"  converting {total_rows} timestamps to datetime (this may take a while)...\")\n",
    "        \n",
    "        for chunk_start in range(0, total_rows, chunk_size):\n",
    "            chunk_end = min(chunk_start + chunk_size, total_rows)\n",
    "            chunk = df.iloc[chunk_start:chunk_end]\n",
    "            \n",
    "            chunk_datetimes = []\n",
    "            for i, row in chunk.iterrows():\n",
    "                dt = parse_time_flexibly(row[day_col], row[time_col])\n",
    "                chunk_datetimes.append(dt)\n",
    "            \n",
    "            datetime_values.extend(chunk_datetimes)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"    processed {chunk_end}/{total_rows} rows ({(chunk_end/total_rows)*100:.1f}%)\")\n",
    "        \n",
    "        df['datetime'] = datetime_values\n",
    "        print(f\"  datetime conversion complete\")\n",
    "    \n",
    "    # calculate magnitude of acceleration\n",
    "    if len(axis_cols) >= 3:\n",
    "        try:\n",
    "            df['accel_magnitude'] = np.sqrt(df[axis_cols[0]]**2 + df[axis_cols[1]]**2 + df[axis_cols[2]]**2)\n",
    "        except Exception as e:\n",
    "            print(f\"  error calculating acceleration magnitude: {str(e)}\")\n",
    "    \n",
    "    # create position indicator from inclinometer columns\n",
    "    if position_cols:\n",
    "        df['position'] = 'unknown'\n",
    "        for col in position_cols:\n",
    "            position_name = col.split(' ')[-1].lower() if ' ' in col else 'position_' + col.lower()\n",
    "            try:\n",
    "                df.loc[df[col] == 1, 'position'] = position_name\n",
    "            except Exception as e:\n",
    "                print(f\"  error setting position from {col}: {str(e)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# main preprocessing function\n",
    "def preprocess_data(data):\n",
    "    processed_data = {}\n",
    "    \n",
    "    # process user_info\n",
    "    if 'user_info' in data and isinstance(data['user_info'], pd.DataFrame) and not data['user_info'].empty:\n",
    "        processed_data['user_info'] = data['user_info'].copy()\n",
    "        print(\"  preprocessed user_info data\")\n",
    "    \n",
    "    # process sleep data\n",
    "    if 'sleep' in data and isinstance(data['sleep'], pd.DataFrame) and not data['sleep'].empty:\n",
    "        processed_data['sleep'] = preprocess_sleep_data(data['sleep'])\n",
    "        print(\"  preprocessed sleep data\")\n",
    "    \n",
    "    # process rr data and extract hrv features\n",
    "    if 'rr' in data and isinstance(data['rr'], pd.DataFrame) and not data['rr'].empty:\n",
    "        processed_data['rr'] = data['rr'].copy()\n",
    "        print(\"  preprocessing RR data and extracting HRV features...\")\n",
    "        processed_data['hrv_features'] = extract_hrv_features(data['rr'])\n",
    "        print(\"  completed HRV feature extraction\")\n",
    "    \n",
    "    # process questionnaire data\n",
    "    if 'questionnaire' in data and isinstance(data['questionnaire'], pd.DataFrame) and not data['questionnaire'].empty:\n",
    "        processed_data['questionnaire'] = data['questionnaire'].copy()\n",
    "        print(\"  preprocessed questionnaire data\")\n",
    "    \n",
    "    # process activity data\n",
    "    if 'activity' in data and isinstance(data['activity'], pd.DataFrame) and not data['activity'].empty:\n",
    "        processed_data['activity'] = preprocess_activity_data(data['activity'])\n",
    "        print(\"  preprocessed activity data\")\n",
    "    \n",
    "    # process actigraph data\n",
    "    if 'actigraph' in data and isinstance(data['actigraph'], pd.DataFrame) and not data['actigraph'].empty:\n",
    "        processed_data['actigraph'] = preprocess_actigraph_data(data['actigraph'])\n",
    "        print(\"  preprocessed actigraph data\")\n",
    "    \n",
    "    # process saliva data\n",
    "    if 'saliva' in data and isinstance(data['saliva'], pd.DataFrame) and not data['saliva'].empty:\n",
    "        processed_data['saliva'] = data['saliva'].copy()\n",
    "        print(\"  preprocessed saliva data\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# execute the data loading and preprocessing\n",
    "print(\"starting data preprocessing...\")\n",
    "\n",
    "# load all user data\n",
    "all_data = load_all_users()\n",
    "\n",
    "# check for missing values in combined data\n",
    "missing_values = check_missing_values(all_data['combined'])\n",
    "\n",
    "# preprocess data for each user\n",
    "preprocessed_individual = []\n",
    "for i, user_data in enumerate(all_data['individual']):\n",
    "    print(f\"preprocessing data for user {i+1}/{len(all_data['individual'])}...\")\n",
    "    preprocessed_user_data = preprocess_data(user_data)\n",
    "    preprocessed_individual.append(preprocessed_user_data)\n",
    "\n",
    "# preprocess combined data\n",
    "preprocessed_combined = preprocess_data(all_data['combined'])\n",
    "\n",
    "# store all preprocessed data\n",
    "preprocessed_data = {\n",
    "    'combined': preprocessed_combined,\n",
    "    'individual': preprocessed_individual,\n",
    "    'missing_values': missing_values\n",
    "}\n",
    "\n",
    "print(\"data preprocessing complete!\")\n",
    "\n",
    "# display summary of the dataset\n",
    "print(\"\\ndataset summary:\")\n",
    "for key, df in preprocessed_combined.items():\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        print(f\"{key}: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# example visualization: distribution of sleep quality metrics\n",
    "if 'sleep' in preprocessed_combined and not preprocessed_combined['sleep'].empty:\n",
    "    print(\"\\ngenerating sleep quality visualizations...\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # find sleep quality metrics with flexible column name matching\n",
    "    sleep_metrics = []\n",
    "    for col in preprocessed_combined['sleep'].columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'sleep time' in col_lower or 'tst' in col_lower:\n",
    "            sleep_metrics.append(col)\n",
    "        elif 'fragmentation index' in col_lower:\n",
    "            sleep_metrics.append(col)\n",
    "        elif 'sleep_efficiency' in col_lower:\n",
    "            sleep_metrics.append(col)\n",
    "        elif 'awakenings' in col_lower and 'number' in col_lower:\n",
    "            sleep_metrics.append(col)\n",
    "    \n",
    "    # limit to 4 metrics for visualization\n",
    "    sleep_metrics = sleep_metrics[:4]\n",
    "    \n",
    "    for i, metric in enumerate(sleep_metrics):\n",
    "        if i < 4:  # ensure we don't exceed the 2x2 grid\n",
    "            plt.subplot(2, 2, i+1)\n",
    "            sns.histplot(preprocessed_combined['sleep'][metric].dropna(), kde=True)\n",
    "            plt.title(f'distribution of {metric}')\n",
    "            plt.xlabel(metric)\n",
    "            plt.ylabel('count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sleep_quality_distributions.png')\n",
    "    print(\"sleep quality visualizations saved to 'sleep_quality_distributions.png'\")\n",
    "    plt.close()\n",
    "\n",
    "# save preprocessed data for future use\n",
    "import pickle\n",
    "\n",
    "with open('mmash_preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "print(\"preprocessed data saved to 'mmash_preprocessed_data.pkl'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
